# -*- coding: utf-8 -*-
"""NLP_CharacterSummary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19IoMuRLCel382L5assQX70mTQZSzBvWu
"""

from transformers import BartTokenizer, BartForConditionalGeneration

def summarize_character(story: str, character: str,
                        max_input_length: int = 1024,
                        summary_max_length: int = 150,
                        summary_min_length: int = 30,
                        num_beams: int = 4) -> str:
    """
    Dynamically summarize a character from a story.

    Parameters:
      story (str): The full story or text from which to extract the character summary.
      character (str): The character to summarize.
      max_input_length (int): Maximum input token length for the model (default 1024).
      summary_max_length (int): Maximum token length for the generated summary.
      summary_min_length (int): Minimum token length for the generated summary.
      num_beams (int): Beam search size (controls the quality of generation).

    Returns:
      str: A summary of the character.
    """
    # Load the model and tokenizer (you may move these outside the function if reused many times)
    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

    # Create a prompt to instruct the model to focus on the character
    prompt = f"Summarize the character '{character}' in the following story:\n{story}"

    # Tokenize the input text with truncation
    inputs = tokenizer(prompt, max_length=max_input_length, truncation=True, return_tensors="pt")

    # Generate the summary using beam search
    summary_ids = model.generate(
        inputs['input_ids'],
        max_length=summary_max_length,
        min_length=summary_min_length,
        length_penalty=2.0,
        num_beams=num_beams,
        early_stopping=True
    )

    # Decode the summary tokens into a human-readable string
    character_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return character_summary

from transformers import T5ForConditionalGeneration, T5Tokenizer
import sys

def dynamic_character_summary(story: str, character: str,
                              max_input_length: int = 512,
                              summary_max_length: int = 100,
                              summary_min_length: int = 30,
                              num_beams: int = 4) -> str:
    """
    Generates a summary focusing on a specific character from a given story.
    """
    # Load T5 tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained('t5-base')
    model = T5ForConditionalGeneration.from_pretrained('t5-base')

    # Construct a prompt explicitly directing the model to focus on the character.
    prompt = f"Summarize the character '{character}' in the following story: {story}"

    # Tokenize the prompt, ensuring truncation for overly long inputs.
    inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=max_input_length, truncation=True)

    # Generate the summary using beam search
    summary_ids = model.generate(
        inputs,
        max_length=summary_max_length,
        min_length=summary_min_length,
        num_beams=num_beams,
        early_stopping=True
    )

    # Convert the generated tokens back to text
    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary_text

if __name__ == "__main__":
    # Check if we have valid command-line arguments.
    # Jupyter/Colab usually passes extra args (-f ...) that we don't want.
    if len(sys.argv) >= 3 and sys.argv[1] != "-f":
        character = sys.argv[1]
        story = sys.argv[2]
    else:
        # When running in a notebook, use default inputs or ask the user interactively.
        print("No valid command-line arguments detected. Using default story and character.")
        story = (
            "Frodo Baggins, a humble hobbit of the Shire, is entrusted with a perilous quest "
            "to destroy a powerful ring that could bring ruin to the world. Throughout his journey, "
            "Frodo faces immense challenges and grows in courage and resilience."
        )
        character = "Frodo Baggins"

    # Generate the character summary using the dynamic function
    character_summary = dynamic_character_summary(story, character)

    print(f"Character Summary of '{character}':\n{character_summary}")

from transformers import T5ForConditionalGeneration, T5Tokenizer

def dynamic_character_summary(story: str, character: str,
                              max_input_length: int = 512,
                              summary_max_length: int = 100,
                              summary_min_length: int = 30,
                              num_beams: int = 4) -> str:
    """
    Generates a summary focusing on a specific character from a given story.
    """
    tokenizer = T5Tokenizer.from_pretrained('t5-base')
    model = T5ForConditionalGeneration.from_pretrained('t5-base')

    prompt = f"Summarize the character '{character}' in the following story: {story}"
    inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=max_input_length, truncation=True)

    summary_ids = model.generate(
        inputs,
        max_length=summary_max_length,
        min_length=summary_min_length,
        num_beams=num_beams,
        early_stopping=True
    )

    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary_text

if __name__ == "__main__":
    # Directly ask for input instead of relying on sys.argv
    story = input("Enter the story text:\n")
    character = input("Enter the character name:\n")

    character_summary = dynamic_character_summary(story, character)
    print(f"\nCharacter Summary of '{character}':\n{character_summary}")

import spacy
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load spaCy for named entity recognition
nlp = spacy.load("en_core_web_sm")

# Load model and tokenizer
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def extract_characters(text):
    doc = nlp(text)
    characters = list({ent.text for ent in doc.ents if ent.label_ == "PERSON"})
    return characters

def get_character_sentences(text, character):
    doc = nlp(text)
    relevant_sentences = [sent.text.strip() for sent in doc.sents if character.lower() in sent.text.lower()]
    return " ".join(relevant_sentences)

def summarize_character(text, character):
    character_text = get_character_sentences(text, character)
    if not character_text:
        return f"No information found for {character}."

    prompt = f"Summarize the personality, role, and actions of the character '{character}' based on the following story:\n\n{character_text}"

    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    outputs = model.generate(inputs["input_ids"], max_length=100, min_length=30, do_sample=False)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def summarize_all_characters(text):
    character_list = extract_characters(text)
    summaries = {}
    for character in character_list:
        print(f"Generating summary for: {character}")
        summaries[character] = summarize_character(text, character)
    return summaries

# Example story
story = """
In the ancient kingdom of Veridia, there lived a resourceful warrior named Gareth. Amidst political strife and looming invasions, Gareth emerged as a beacon of hope.
His companion, Elara, a wise and intuitive mage, assisted him in deciphering ancient prophecies.
Meanwhile, the cunning Lord Malrec plotted in the shadows to seize power, and the gentle healer, Mirin, devoted herself to caring for the wounded.
As the kingdom fell into chaos, each individual's actions shaped the fate of Veridia.
"""

summaries = summarize_all_characters(story)
for name, summary in summaries.items():
    print(f"\nCharacter Summary for {name}:\n{summary}\n")

