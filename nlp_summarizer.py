# -*- coding: utf-8 -*-
"""NLP_Summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OiTVuchcQ35N6kllv1tdtn8bxWSFE3_K
"""

pip install transformers datasets torch

"""Loading Dataset

"""

from datasets import load_dataset

# Load the CNN/DailyMail dataset
dataset = load_dataset("cnn_dailymail", "3.0.0")

# Take a sample for testing
train_data = dataset['train']
print(train_data[:4]['article'])   # The full article
print(train_data[:4]['highlights'])  # The summary

"""Load Pretrained Model and Tokenizer

"""

from transformers import BartTokenizer, BartForConditionalGeneration

# Load tokenizer and model
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')

"""Tokenize Inputs"""

article_text = input()

inputs = tokenizer(
    article_text,
    max_length=1024,
    return_tensors="pt",
    truncation=True
)

"""Generate Summary"""

summary_ids = model.generate(
    inputs["input_ids"],
    max_length=150,
    min_length=40,
    length_penalty=2.0,
    num_beams=7,
    early_stopping=True
)

# Decode summary
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(summary)

"""Fine-tuning (Optional â€” Full Training Loop)"""

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# Preprocess all data
def preprocess_function(examples):
    inputs = tokenizer(examples["article"], max_length=1024, truncation=True)
    targets = tokenizer(examples["highlights"], max_length=128, truncation=True)
    inputs["labels"] = targets["input_ids"]
    return inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=False,  # Set to True if using a GPU with mixed precision
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"].select(range(1000)),  # Use smaller set for quick training
    eval_dataset=tokenized_dataset["validation"].select(range(100)),
    tokenizer=tokenizer,
)

# Start training
trainer.train()

"""Save the Fine-tuned Model"""

model.save_pretrained("summarizer-bart")
tokenizer.save_pretrained("summarizer-bart")

"""Evaluate the Model"""

# You can use rouge_score for evaluation
from datasets import load_metric
rouge = load_metric("rouge")

# Evaluate on a few examples
predictions = []
references = []

for example in dataset['validation'].select(range(10)):
    inputs = tokenizer(example['article'], return_tensors="pt", truncation=True, max_length=1024)
    ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4)
    pred = tokenizer.decode(ids[0], skip_special_tokens=True)

    predictions.append(pred)
    references.append(example['highlights'])

results = rouge.compute(predictions=predictions, references=references)
print(results)